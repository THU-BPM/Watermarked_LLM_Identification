# Copyright 2024 THU BPM
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

def get_first_token_logits(model, tokenizer, prompt, vocab_size, device):
    """Gets the logits for the first token generated by the model."""
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    outputs = model(**inputs)
    logits = outputs.logits
    return logits[:, -1, :vocab_size].cpu()


def get_logits_dict(
    prompt: str,
    context_window: str,
    generation_model: AutoModelForCausalLM,
    generation_tokenizer: AutoTokenizer,
    vocab_size: int,
    device: str,
):
    """Get the logits dictionary for the context window."""
    # Get the ids of the prompt
    input_ids = generation_tokenizer.encode(
        prompt, return_tensors="pt", add_special_tokens=True
    ).to(device)

    # Get the last segment of the input string & ids
    post_input_str = prompt.rsplit("\n", 1)[-1]
    post_input_ids = generation_tokenizer.encode(
        post_input_str, add_special_tokens=False
    )

    # Get the last segment of the input string & ids
    main_prompt_str = post_input_str + context_window
    main_prompt_ids = generation_tokenizer.encode(
        main_prompt_str, add_special_tokens=False
    )

    # Get the ids of the context window (to prevent the tokenizer from handling the prefix spaces differently, use the length of main_prompt_ids to slice)
    context_window_ids = main_prompt_ids[len(post_input_ids) :]

    print(f"Length of context window ids: {len(context_window_ids)}")

    gen_logits = []

    with torch.no_grad():
        for id in context_window_ids:
            # Get the current input_ids
            input_ids = torch.cat([input_ids, torch.tensor([[id]]).to(device)], dim=-1)
            # Generate logits
            output = generation_model(input_ids)
            logits = output.logits[:, -1, :vocab_size].cpu().detach()
            gen_logits.append(logits)

    def create_nested_dict(ids, logits):
        if not ids:
            return {}

        current_id = ids[0]

        # If the ID array only has one element, create a dictionary containing 'logits'
        if len(ids) == 1:
            return {current_id: {"logits": logits[0]}}

        # Recursively call to create the next level nested dictionary
        return {
            current_id: {"logits": logits[0], **create_nested_dict(ids[1:], logits[1:])}
        }

    nested_dict = create_nested_dict(context_window_ids, gen_logits)
    return nested_dict


def merge_nested_dicts(dict1, dict2):
    """
    Recursively merge two nested dictionaries, and the result is saved in dict1.
    """
    for key in dict2:
        if key in dict1:
            if isinstance(dict1[key], dict) and isinstance(dict2[key], dict):
                # If both dictionaries have sub-dictionaries on the same key, recursively merge them
                merge_nested_dicts(dict1[key], dict2[key])
            else:
                # If the keys are the same but the values are not dictionaries, directly overwrite the value of dict1 with the value of dict2
                dict1[key] = dict2[key]
        else:
            # If dict1 does not have this key, directly add it
            dict1[key] = dict2[key]


def update_root_dict_with_nested(root_dict, nested_dict):
    """
    Update the root dictionary with multiple nested dictionaries.
    """
    merge_nested_dicts(root_dict, nested_dict)
    return root_dict